---
permalink: disaster-recovery/task_reassign_pool1_disastersite_mcip.html 
sidebar: sidebar 
keywords: metrocluster, disaster, site, replacement, controller, module, reassign, ownership 
summary: Se i moduli controller o le schede NVRAM sono stati sostituiti nel sito di emergenza, è necessario riassegnare i dischi appartenenti agli aggregati root ai moduli controller sostitutivi. 
---
= Riassegnazione della proprietà dei dischi per il pool 1 nel sito di emergenza (configurazioni MetroCluster IP)
:allow-uri-read: 
:icons: font
:imagesdir: ../media/


[role="lead"]
Se uno o entrambi i moduli controller o le schede NVRAM sono stati sostituiti nel sito di emergenza, l'ID del sistema è stato modificato ed è necessario riassegnare i dischi appartenenti agli aggregati root ai moduli controller sostitutivi.

.A proposito di questa attività
Poiché i nodi sono in modalità switchover, solo i dischi contenenti gli aggregati root del pool1 del sito di disastro verranno riassegnati in questa attività. Si tratta degli unici dischi ancora di proprietà del vecchio ID di sistema a questo punto.

Questa attività viene eseguita sui nodi sostitutivi del sito di emergenza.

Questa attività viene eseguita in modalità manutenzione.

Gli esempi fanno le seguenti ipotesi:

* Il sito A è il sito di disastro.
* Il nodo_A_1 è stato sostituito.
* Il nodo_A_2 è stato sostituito.
* Il sito B è il sito sopravvissuto.
* Node_B_1 è integro.
* Node_B_2 è integro.


Gli ID di sistema vecchi e nuovi sono stati identificati in link:../disaster-recovery/task_replace_hardware_and_boot_new_controllers.html#determining-the-system-ids-of-the-replacement-controller-modules["Determinazione dei nuovi ID di sistema dei moduli controller sostitutivi"].

Gli esempi di questa procedura utilizzano controller con i seguenti ID di sistema:

|===


| Nodo | ID di sistema originale | Nuovo ID di sistema 


 a| 
Node_A_1
 a| 
4068741258
 a| 
1574774970



 a| 
Node_A_2
 a| 
4068741260
 a| 
1574774991



 a| 
Node_B_1
 a| 
4068741254
 a| 
invariato



 a| 
Node_B_2
 a| 
4068741256
 a| 
invariato

|===
.Fasi
. Con il nodo sostitutivo in modalità manutenzione, riassegnare i dischi aggregati root, utilizzando il comando corretto, a seconda che il sistema sia configurato con ADP e la versione di ONTAP.
+
È possibile procedere con la riassegnazione quando richiesto.

+
|===


| Se il sistema utilizza ADP... | Utilizzare questo comando per la riassegnazione del disco... 


 a| 
Sì (ONTAP 9.8)
 a| 
`disk reassign -s old-system-ID -d new-system-ID -r dr-partner-system-ID`



 a| 
Sì (ONTAP 9.7.x e versioni precedenti)
 a| 
`disk reassign -s old-system-ID -d new-system-ID -p old-partner-system-ID`



 a| 
No
 a| 
`disk reassign -s old-system-ID -d new-system-ID`

|===
+
L'esempio seguente mostra la riassegnazione dei dischi su un sistema non ADP:

+
[listing]
----
*> disk reassign -s 4068741256 -d 1574774970
Partner node must not be in Takeover mode during disk reassignment from maintenance mode.
Serious problems could result!!
Do not proceed with reassignment if the partner is in takeover mode. Abort reassignment (y/n)? n

After the node becomes operational, you must perform a takeover and giveback of the HA partner node to ensure disk reassignment is successful.
Do you want to continue (y/n)? y
Disk ownership will be updated on all disks previously belonging to Filer with sysid 537037643.
Do you want to continue (y/n)? y
disk reassign parameters: new_home_owner_id 537070473 , new_home_owner_name
Disk 0m.i0.3L14 will be reassigned.
Disk 0m.i0.1L6 will be reassigned.
Disk 0m.i0.1L8 will be reassigned.
Number of disks to be reassigned: 3
----
. Distruggere il contenuto dei dischi della mailbox:
+
`mailbox destroy local`

+
Quando richiesto, è possibile procedere con l'operazione Destroy.

+
L'esempio seguente mostra l'output per il comando local di Destroy della mailbox:

+
[listing]
----
*> mailbox destroy local
Destroying mailboxes forces a node to create new empty mailboxes,
which clears any takeover state, removes all knowledge
of out-of-date plexes of mirrored volumes, and will prevent
management services from going online in 2-node cluster
HA configurations.
Are you sure you want to destroy the local mailboxes? y
...............Mailboxes destroyed.
*>
----
. Se i dischi sono stati sostituiti, ci saranno dei plessi locali guasti che devono essere cancellati.
+
.. Visualizzare lo stato dell'aggregato:
+
`aggr status`

+
Nell'esempio seguente, il nodo plex_A_1_aggr0/plex0 non è riuscito.

+
[listing]
----
*> aggr status
Aug 18 15:00:07 [node_B_1:raid.vol.mirror.degraded:ALERT]: Aggregate node_A_1_aggr0 is
   mirrored and one plex has failed. It is no longer protected by mirroring.
Aug 18 15:00:07 [node_B_1:raid.debug:info]: Mirrored aggregate node_A_1_aggr0 has plex0
   clean(-1), online(0)
Aug 18 15:00:07 [node_B_1:raid.debug:info]: Mirrored aggregate node_A_1_aggr0 has plex2
   clean(0), online(1)
Aug 18 15:00:07 [node_B_1:raid.mirror.vote.noRecord1Plex:error]: WARNING: Only one plex
   in aggregate node_A_1_aggr0 is available. Aggregate might contain stale data.
Aug 18 15:00:07 [node_B_1:raid.debug:info]: volobj_mark_sb_recovery_aggrs: tree:
   node_A_1_aggr0 vol_state:1 mcc_dr_opstate: unknown
Aug 18 15:00:07 [node_B_1:raid.fsm.commitStateTransit:debug]: /node_A_1_aggr0 (VOL):
   raid state change UNINITD -> NORMAL
Aug 18 15:00:07 [node_B_1:raid.fsm.commitStateTransit:debug]: /node_A_1_aggr0 (MIRROR):
   raid state change UNINITD -> DEGRADED
Aug 18 15:00:07 [node_B_1:raid.fsm.commitStateTransit:debug]: /node_A_1_aggr0/plex0
   (PLEX): raid state change UNINITD -> FAILED
Aug 18 15:00:07 [node_B_1:raid.fsm.commitStateTransit:debug]: /node_A_1_aggr0/plex2
   (PLEX): raid state change UNINITD -> NORMAL
Aug 18 15:00:07 [node_B_1:raid.fsm.commitStateTransit:debug]: /node_A_1_aggr0/plex2/rg0
   (GROUP): raid state change UNINITD -> NORMAL
Aug 18 15:00:07 [node_B_1:raid.debug:info]: Topology updated for aggregate node_A_1_aggr0
   to plex plex2
*>
----
.. Eliminare il plesso guasto:
+
`aggr destroy plex-id`

+
[listing]
----
*> aggr destroy node_A_1_aggr0/plex0
----


. Arrestare il nodo per visualizzare il prompt DEL CARICATORE:
+
`halt`

. Ripetere questi passaggi sull'altro nodo del sito di emergenza.

