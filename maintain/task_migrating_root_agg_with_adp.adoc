---
permalink: maintain/task_migrating_root_agg_with_adp.html 
sidebar: sidebar 
keywords: metrocluster, maintain, service, migrate, root, aggregate, adp, disks, partition, advanced, disk, partitioning 
summary: 'È possibile migrare senza interruzioni un aggregato root esistente che utilizza la partizione avanzata dei dischi (ADP).' 
---
= Migrare un aggregato root configurato con ADP nelle configurazioni IP di MetroCluster
:allow-uri-read: 
:icons: font
:imagesdir: ../media/


È possibile migrare senza interruzioni un aggregato root esistente che utilizza Advanced Disk Partitioning (ADP) se l'aggregato root sta esaurendo lo spazio o se si desidera passare dall'utilizzo di SSD a bassa capacità a SSD a elevata capacità.

.A proposito di questa attività
* La coppia ha (Local High Availability) deve essere abilitata per il failover dello storage.
* Questa procedura è senza interruzioni.
* La configurazione deve essere in uno stato stabile e fornire i dati normalmente, senza interruzioni frequenti.
* Durante questa procedura, non eseguire aggiornamenti dell'hardware o altre operazioni che potrebbero causare interruzioni.
* È possibile migrare un solo aggregato root alla volta. Non tentare di migrare due aggregati root contemporaneamente.


.Fasi
. [[step_1, verify the Health of the Configuration]]Controlla lo stato della configurazione.
+
.. Verificare che MetroCluster sia configurato e in modalità normale su ciascun cluster:
 +
`metrocluster show`
+
[listing]
----
cluster_A::> metrocluster show
Cluster                   Entry Name          State
------------------------- ------------------- -----------
 Local: cluster_A         Configuration state configured
                          Mode                normal
                          AUSO Failure Domain auso-on-cluster-disaster
Remote: cluster_B         Configuration state configured
                          Mode                normal
                          AUSO Failure Domain auso-on-cluster-disaster
----
.. Verificare che il mirroring sia attivato su ciascun nodo:
+
`metrocluster node show`

+
[listing]
----
cluster_A::> metrocluster node show
DR                           Configuration  DR
Group Cluster Node           State          Mirroring Mode
----- ------- -------------- -------------- --------- --------
1     cluster_A
              node_A_1       configured     enabled   normal
      cluster_B
              node_B_1       configured     enabled   normal
2 entries were displayed.
----
.. Verificare che i componenti di MetroCluster siano in buone condizioni:
+
`metrocluster check run`

+
[listing]
----
cluster_A::> metrocluster check run

Last Checked On: 10/1/2014 16:03:37

Component           Result
------------------- ---------
nodes               ok
lifs                ok
config-replication  ok
aggregates          ok
4 entries were displayed.

Command completed. Use the "metrocluster check show -instance" command or sub-commands in "metrocluster check" directory for detailed results.
To check if the nodes are ready to do a switchover or switchback operation, run "metrocluster switchover -simulate" or "metrocluster switchback -simulate", respectively.
----
.. Verificare che non siano presenti avvisi sullo stato di salute:
+
`system health alert show`



. Verificare che la coppia ha locale sia abilitata per il failover dello storage:
+
`storage failover show`

+
L'output dovrebbe indicare che è possibile effettuare il takeover per entrambi i nodi:

+
[listing]
----
cluster_A::> storage failover show
                              Takeover
Node           Partner        Possible State Description
-------------- -------------- -------- ---------------------------
cluster-01     cluster-02      true     Connected to cluster-02

cluster-02     cluster-01      true     Connected to cluster-01
2 entries were displayed.
----
. Azzerare i dischi spare:
+
`run * disk zero spares`

. Identificare la dimensione dell'aggregato root:
+
`node run local aggr status -r <root_agg_name>`

+
Nell'esempio seguente, l'aggregato root ha dieci dischi in "Pool0" e dieci dischi in "Pool1".

+
[listing]
----
cluster_A::*> node run local aggr status -r <root_agg_name>
Aggregate <root_agg_name> (online, raid_dp, mirrored, fast zeroed) (block checksums)
  Plex /<root_agg_name>/plex0 (online, normal, active, pool0)
    RAID group /<root_agg_name>/plex0/rg0 (normal, block checksums, max_wdbn 5767167)

      RAID Disk Device  HA  SHELF BAY CHAN Pool Type  RPM  Used (MB/blks)    Phys (MB/blks)
      --------- ------  ------------- ---- ---- ---- ----- --------------    --------------
      dparity   e2a.11.0.0P3    e2a   11  0   NV:A   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      parity    e10b.11.3.1P3   e10b  11  1   NV:B   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      e2a.11.0.2P3    e2a   11  2   NV:A   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      e2a.11.0.3P3    e2a   11  3   NV:A   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      e2a.11.0.4P3    e2a   11  4   NV:A   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      e10b.11.3.5P3   e10b  11  5   NV:B   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      e10b.11.3.12P3  e10b  11  12  NV:B   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      e10b.11.3.13P3  e10b  11  13  NV:B   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      e10b.11.3.14P3  e10b  11  14  NV:B   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      e10b.11.3.15P3  e10b  11  15  NV:B   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)

  Plex /<root_agg_name>/plex2 (online, normal, active, pool1)
    RAID group /<root_agg_name>/plex2/rg0 (normal, block checksums, max_wdbn 5767167)

      RAID Disk Device  HA  SHELF BAY CHAN Pool Type  RPM  Used (MB/blks)    Phys (MB/blks)
      --------- ------  ------------- ---- ---- ---- ----- --------------    --------------
      dparity   0m.i2.2L1P3     0m    22  5          1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      parity    0m.i1.0L36P3    0m    22  14         1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      0v.i2.2L18P3    0v    22  12         1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      0v.i2.3L17P3    0v    22  2          1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      0v.i1.0L25P3    0v    22  13         1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      0v.i1.0L40P3    0v    22  4          1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      0m.i1.1L39P3    0m    22  17         1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      0m.i1.1L46P3    0m    22  15         1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      0m.i2.3L13P3    0m    22  0          1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      0v.i1.1L26P3    0v    22  1          1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)

cluster_A::*>
----
. Assegnare i dischi container.
+
Prima di assegnare i dischi, verificare che il numero consigliato di dischi di riserva sia assegnato a ciascun nodo. Questi dischi vengono partizionati prima della migrazione dell'aggregato root. Per ulteriori informazioni, vedere link:https://docs.netapp.com/us-en/ontap-metrocluster/install-ip/concept_considerations_drive_assignment.html["Considerazioni sull'assegnazione automatica dei dischi e sui sistemi ADP in ONTAP 9.4 e versioni successive"].

+
Eseguire il seguente comando per assegnare i dischi:

+
`storage disk assign -disklist 1.11.0,1.11.1,…  -owner cluster-01 -pool 0`

. Identificare le dimensioni della partizione root.
+
Le dimensioni della partizione root dipendono dal numero di dischi disponibili per la partizione su ciascun nodo. NetApp consiglia di disporre di almeno 12 dischi per nodo per la partizione.

+
È possibile utilizzare la seguente tabella per determinare il layout dell'aggregato root:

+
[cols="25,75"]
|===
| Numero di dischi da partizionare | Layout aggregato root 


| 4 dischi per nodo | 2 unità dati e 2 unità di parità 


| 12 dischi per nodo | 8 unità dati, 2 unità di parità e 2 unità di riserva 


| 24 dischi per nodo | 20 unità dati, 2 unità di parità e 2 unità di riserva 
|===
+
Per identificare le dimensioni della partizione root, è necessario dividere il numero totale di blocchi 4K in modo uniforme tra tutte le unità dati.

+
Ad esempio, se si dispone di un layout aggregato root di 8 unità dati, 2 unità di parità e 2 unità spare con una dimensione aggregata root di 112958795 blocchi, è necessario dividere 112958795 per 8 per ottenere la dimensione della partizione root.

+
(112958795 / 8) = 14119849.375

+
Una volta arrotondato il valore, la dimensione della partizione root è 14119850.

. Partizione di ciascun disco nell'aggregato root:
+
`cluster_A*> disk partition -n 3 -i 3 -b <root_partition_size> <disk_id>`

. Assegnare le partizioni.
+

NOTE: Nei sistemi che utilizzano ADP, gli aggregati vengono creati utilizzando partizioni in cui ciascun disco viene partizionato nelle partizioni P1, P2 e P3.

+
.. Assegnare la partizione P3 allo stesso nodo proprietario del disco container:
+
`storage disk assign -disk <disk_id> -root true -pool 0 -owner cluster-01`

.. Assegnare la partizione P1 al sistema con il numero ID di sistema inferiore nella coppia ha:
+
`storage disk assign -disk <disk_id> -data1 true -pool 0 -owner cluster-01`

.. Assegnare la partizione P2 al sistema con il numero ID di sistema superiore nella coppia ha:
+
`storage disk assign -disk <disk_name> -data2 true -pool 0 -owner cluster-02`

+
Ripetere questo passaggio per ogni disco partizionato.



. Verificare che sia possibile effettuare il takeover:
+
`storage failover show`

+
[listing]
----
cluster_A::> storage failover show
                              Takeover
Node           Partner        Possible State Description
-------------- -------------- -------- ---------------------------
cluster-01     cluster-02      true     Connected to cluster-02

cluster-02     cluster-01      true     Connected to cluster-01
2 entries were displayed.
----
. Migrare l'aggregato root.
+
Per ciascun nodo, eseguire la migrazione specificando l'elenco dei dischi in Pool0 e il tipo di RAID di destinazione come parametri:

+
`system node migrate-root -node cluster-01 -disklist <pool0_disk_list> -raid-type <target_raid_type>`

+
Ad esempio, se l'aggregato root per "cluster-01" è costituito da dieci dischi con "raid_dp", il seguente comando migra l'aggregato root:

+
[listing]
----
system node migrate-root -node cluster-01 -disklist 1.11.1.P3,1.11.2.P3,1.11.3.P3,1.11.4.P3,1.11.5.P3,1.11.6.P3,1.11.7.P3,1.11.8.P3,1.11.9.P3,1.11.10.P3 -raid-type raid_dp

Warning: This is a partially automated and guided procedure for migrating the
         root aggregate on the node "cluster-01".
         Negotiated switchover is about to start.
         Warning: This operation will create a new root aggregate and replace
         the existing root on the node "cluster-01". The existing root
         aggregate will be discarded.
Do you want to continue? {y|n}: y

Info: Started migrate-root job. Run "job show -id 51 -instance" command to
      check the progress of the job.
      Once the job is complete, mirror the root aggregate using the "storage
      aggregate mirror" command
----
+

IMPORTANT: Se il numero di dischi non è sufficiente, aggiungere altri dischi o scegliere un tipo RAID diverso.

+
Il completamento del processo di migrazione potrebbe richiedere alcuni minuti. Durante la migrazione, il nodo viene riavviato diverse volte e potrebbero verificarsi errori sugli altri nodi. È possibile ignorare questi errori e attendere il completamento del processo di migrazione.

. Se lo si desidera, monitorare l'avanzamento della migrazione.
+
Dal secondo sito, eseguire:

+
`job show -id 51 -instance`

. Riattivare la partizione automatica RAID per tutti i nodi IP MetroCluster:
+
`storage raidlm policy modify -node <node> -policy-name auto_partition_ssds_post_init -policy-type Shared-Disk -is-enable true`

. Verificare che la migrazione sia stata eseguita correttamente:
+
`run local aggr status -r <root_agg_name>`

+
[listing]
----
cluster_A::*> node run local aggr status -r <root_agg_name>
Aggregate <root_agg_name> (online, raid0, fast zeroed) (block checksums)
  Plex /<root_agg_name>/plex0 (online, normal, active, pool0)
    RAID group /<root_agg_name>/plex0/rg0 (normal, block checksums, max_wdbn 6127616)

      RAID Disk Device  HA  SHELF BAY CHAN Pool Type  RPM  Used (MB/blks)    Phys (MB/blks)
      --------- ------  ------------- ---- ---- ---- ----- --------------    --------------
      data      e2a.11.0.16P3   e2a   11  16  NV:A   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      e10b.11.3.17P3  e10b  11  17  NV:B   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)

cluster_A::*>
----
. Ripetere i passi da a. <<step_1,verificare lo stato della configurazione>>.

